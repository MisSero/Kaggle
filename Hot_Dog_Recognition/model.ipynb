{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from random import choice\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HotdogOrNotDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None, use_albumentations=False):\n",
    "        self.transform = transform\n",
    "        self.folder = folder\n",
    "        self.name_data = os.listdir(folder)\n",
    "        self.use_albumentations = use_albumentations\n",
    "    def __len__(self):\n",
    "        return len(self.name_data)\n",
    "    \n",
    "    def __getitem__(self, index):        \n",
    "        img_id = self.name_data[index]\n",
    "        img = Image.open(os.path.join(self.folder, img_id))\n",
    "        \n",
    "        frame = img_id.find('_')\n",
    "        name = img_id[:frame]\n",
    "        y = 0\n",
    "        if name in (set(['frankfurter', 'chili-dog', 'hotdog'])):\n",
    "            y = 1\n",
    "\n",
    "        if self.transform:\n",
    "            if self.use_albumentations:\n",
    "                image = self.transform(image=np.array(img))\n",
    "                img = image['image']\n",
    "            else:\n",
    "                img = self.transform(img)\n",
    "            \n",
    "    \n",
    "        return img, y, img_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(dataset, indices, title=None, count=10):\n",
    "    plt.figure(figsize=(count*3,3))\n",
    "    display_indices = indices[:count]\n",
    "    if title:\n",
    "        plt.suptitle(\"%s %s/%s\" % (title, len(display_indices), len(indices)))        \n",
    "    for i, index in enumerate(display_indices):    \n",
    "        x, y, _ = dataset[index]\n",
    "        plt.subplot(1,count,i+1)\n",
    "        plt.title(\"Label: %s\" % y)\n",
    "        plt.imshow(x)\n",
    "        plt.grid(False)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_and_f1(model, loader):\n",
    "    model.eval()\n",
    "    \n",
    "    correct_samples = 0\n",
    "    total_samples = 0\n",
    "    total_f1 = 0\n",
    "    y_full = []\n",
    "    prediction_full = []\n",
    "    for _, (x, y, _) in enumerate(loader):\n",
    "        x_gpu = x.to(device)\n",
    "        y_gpu = y.to(device)\n",
    "        prediction = model(x_gpu)\n",
    "        indices = torch.argmax(prediction, 1)\n",
    "        correct_samples += torch.sum(indices == y_gpu)\n",
    "        total_samples += y.shape[0]\n",
    "        y_full.extend(y.tolist())\n",
    "        prediction_full.extend(indices.tolist())\n",
    "    val_accuracy = float(correct_samples) / total_samples\n",
    "    total_f1 = f1_score(y_full, prediction_full)\n",
    "    \n",
    "    return val_accuracy, total_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss, optimizer, num_epochs, scheduler):    \n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    val_f1_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'--Epoch {epoch+1}/{num_epochs}, lr = {scheduler.get_last_lr()[-1]}')\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        loss_accum = 0\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for i_step, (x, y,_) in enumerate(train_loader):\n",
    "          \n",
    "            x_gpu = x.to(device)\n",
    "            y_gpu = y.to(device)\n",
    "            prediction = model(x_gpu)    \n",
    "            loss_value = loss(prediction, y_gpu)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, indices = torch.max(prediction, 1)\n",
    "            correct_samples += torch.sum(indices == y_gpu)\n",
    "            total_samples += y.shape[0]\n",
    "            \n",
    "            loss_accum += loss_value\n",
    "        scheduler.step()\n",
    "\n",
    "        ave_loss = loss_accum / i_step\n",
    "        train_accuracy = float(correct_samples) / total_samples\n",
    "        val_accuracy, val_f1 = compute_accuracy_and_f1(model, val_loader)\n",
    "        \n",
    "        loss_history.append(float(ave_loss))\n",
    "        train_history.append(train_accuracy)\n",
    "        val_history.append(val_accuracy)\n",
    "        val_f1_history.append(val_f1)\n",
    "        \n",
    "        print(\"----Average loss: %f, Train accuracy: %f, Val accuracy: %f, Val f1 score: %f\" % (ave_loss, train_accuracy, val_accuracy, val_f1))\n",
    "        \n",
    "    return loss_history, train_history, val_history, val_f1_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepaire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfs_for_vis = albumentations.Compose([\n",
    "    albumentations.RandomRotate90(p=0.3),\n",
    "    albumentations.Blur(p=0.1),\n",
    "    albumentations.RandomBrightness(p=0.3),\n",
    "    albumentations.GaussNoise(p=0.3),\n",
    "    albumentations.ShiftScaleRotate(p=0.3),\n",
    "    albumentations.JpegCompression(quality_lower=90, p=0.3),\n",
    "    ])\n",
    "\n",
    "tfs = albumentations.Compose([\n",
    "    albumentations.RandomRotate90(p=0.3),\n",
    "    albumentations.Blur(p=0.1),\n",
    "    albumentations.RandomBrightness(p=0.3),\n",
    "    albumentations.GaussNoise(p=0.3),\n",
    "    albumentations.ShiftScaleRotate(p=0.3),\n",
    "    albumentations.JpegCompression(quality_lower=90, p=0.3),\n",
    "    albumentations.Resize(224, 224),\n",
    "    albumentations.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    albumentations.torch.ToTensor(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HotdogOrNotDataset('data/train.lnk', \n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Resize((224, 224)),\n",
    "                           #transforms.ColorJitter(hue=.50, saturation=.50),\n",
    "                           transforms.RandomVerticalFlip(),\n",
    "                           transforms.RandomRotation(50),\n",
    "                           transforms.ToTensor(),\n",
    "                           # Use mean and std for pretrained models\n",
    "                           # https://pytorch.org/docs/stable/torchvision/models.html\n",
    "                           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])                         \n",
    "                       ])\n",
    "                      )\n",
    "test_dataset = HotdogOrNotDataset('data/test.lnk', \n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Resize((224, 224)),\n",
    "                           transforms.ToTensor(),\n",
    "                           # Use mean and std for pretrained models\n",
    "                           # https://pytorch.org/docs/stable/torchvision/models.html\n",
    "                           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])                         \n",
    "                       ])\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.choice(np.arange(len(train_dataset)), 10, replace=False)\n",
    "\n",
    "visualize_samples(train_dataset, indices, \"Samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "data_size = len(train_dataset)\n",
    "validation_fraction = .2\n",
    "\n",
    "\n",
    "val_split = int(np.floor((validation_fraction) * data_size))\n",
    "indices = list(range(data_size))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "val_indices, train_indices = indices[:val_split], indices[val_split:]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                         sampler=val_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'lr': [1e-2, 1e-3, 1e-4],\n",
    "          'wd': [1e-2, 1e-3, 1e-4, 1e-5],\n",
    "          'gamma': [0.6, 0.8, 0.9]}\n",
    "\n",
    "iterations = 4\n",
    "best_model = None\n",
    "best_val_f1 = 0\n",
    "best_lr = None\n",
    "best_wd = None\n",
    "best_gamma = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iterations):\n",
    "    lr = choice(params['lr'])\n",
    "    wd = choice(params['wd'])\n",
    "    gamma = choice(params['gamma'])\n",
    "    \n",
    "    print(f'Iteration {i+1}/{iterations}: lr = {lr}, wd = {wd}, gamma = {gamma}')\n",
    "    \n",
    "    model = models.resnte50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "    # for param in model.parameters():\n",
    "    #     param.requires_grad = False\n",
    "        \n",
    "    num_ft = model.fc.in_features\n",
    "    \n",
    "    model.fc = nn.Linear(num_ft, 2)\n",
    "    model.to(device)\n",
    "    model_params = model.fc.parameters()\n",
    "\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': [param for name, param in model.state_dict().items()\n",
    "                    if not 'fc' in name]},\n",
    "        {'params': model_params, 'lr': lr, 'weight_decay': wd}],\n",
    "                           lr=lr*1e-2, weight_decay=wd*1e-2)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, 2, gamma=gamma)\n",
    "    loss_history, train_history, val_history, val_f1_history = train_model(model, train_loader, val_loader, loss, optimizer, 3, scheduler)\n",
    "    f1_model = val_f1_history[-1]\n",
    "    if f1_model> best_val_f1:\n",
    "        best_val_f1 = f1_model\n",
    "        best_model = model\n",
    "        best_lr = lr\n",
    "        best_wd = wd\n",
    "        best_gamma = gamma"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
